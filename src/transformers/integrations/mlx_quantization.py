# Copyright 2026 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
MLX affine quantization integration for transformers.

This module provides:
  - ``MlxLinear``: a drop-in replacement for ``nn.Linear`` that stores weights
    as affine-quantized uint32 packed tensors and uses the ``quantization-mlx``
    Metal kernels for the forward pass.
  - ``replace_with_mlx_linear``: walks a model and swaps every eligible
    ``nn.Linear`` with ``MlxLinear``.
  - ``MlxQuantize`` / ``MlxDequantize``: weight conversion operations that
    participate in the new ``WeightConverter`` pipeline.

Weight layout (transposed, matching ``affine_qmm_t``):
  - ``weight``: ``[N, K_packed]`` (``uint32``) — K is the packed dimension.
  - ``scales``:  ``[N, K // group_size]`` (``float16 / bfloat16``)
  - ``qbiases``: ``[N, K // group_size]`` (same dtype as scales)

The kernel call is ``affine_qmm_t(x, weight, scales, qbiases, group_size, bits)``
which computes ``y = x @ dequant(weight).T``, identical to ``nn.Linear``.
"""

from ..core_model_loading import ConversionOps
from ..quantizers.quantizers_utils import should_convert_module
from ..utils import is_torch_available, logging


if is_torch_available():
    import torch
    import torch.nn as nn


logger = logging.get_logger(__name__)

# Lazily loaded kernel module from the Hub
_mlx_kernel = None


def _get_mlx_kernel():
    """Lazily load the quantization-mlx kernel from Hugging Face Hub."""
    global _mlx_kernel
    if _mlx_kernel is None:
        try:
            from .hub_kernels import get_kernel

            _mlx_kernel = get_kernel("medmekk/quantization-mlx")
        except Exception as e:
            raise ImportError(
                f"Failed to load the quantization-mlx kernel from the Hub: {e}. "
                "Make sure you have `kernels` installed (`pip install kernels`) "
                "and are running on an Apple Silicon machine."
            ) from e
    return _mlx_kernel


# ---------------------------------------------------------------------------
# MlxLinear — the quantized nn.Linear replacement
# ---------------------------------------------------------------------------


class MlxLinear(nn.Linear):
    """
    A quantized linear layer that stores weights in affine uint32 packed format
    and uses the ``quantization-mlx`` Metal kernels for the forward pass.

    Parameters match ``nn.Linear`` with additional quantization metadata.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = False,
        dtype=torch.uint32,
        bits: int = 4,
        group_size: int = 128,
    ):

        nn.Module.__init__(self)

        self.in_features = in_features
        self.out_features = out_features
        self.bits = bits
        self.group_size = group_size

        elems_per_int = 32 // bits
        k_packed = in_features // elems_per_int
        n_groups = in_features // group_size

        # default dtype=torch.uint32 is used for
        # pre-quantized loading (packed uint32 weights from checkpoint).
        # When quantize-on-the-fly, replace_with_mlx_linear passes dtype=None
        # which creates a full-shape float weight so the loader can fill it
        # with the original checkpoint values before MlxQuantize converts them.
        if dtype == torch.uint32:
            # Pre-quantized: packed shape [N, K_packed] in uint32
            self.weight = nn.Parameter(
                torch.zeros(out_features, k_packed, dtype=torch.uint32), requires_grad=False
            )
        else:
            # Quantize-on-the-fly: full shape [N, K] in the original dtype
            # (dtype=None → PyTorch default float). The MlxQuantize ConversionOp
            # will transform this into packed uint32 + scales + qbiases.
            self.weight = nn.Parameter(
                torch.zeros(out_features, in_features, dtype=dtype), requires_grad=False
            )

        # For pre-quantized (dtype==uint32): scales/qbiases will be loaded
        # from the checkpoint in whatever dtype they were saved as.
        # For quantize-on-the-fly (dtype==None/float): MlxQuantize will set
        # them in the original weight's dtype (e.g. bfloat16).
        # We use float16 as the placeholder — it gets overwritten either way.
        scales_dtype = torch.float16 if dtype == torch.uint32 else (dtype or torch.float16)
        self.scales = nn.Parameter(
            torch.zeros(out_features, n_groups, dtype=scales_dtype), requires_grad=False
        )
        self.qbiases = nn.Parameter(
            torch.zeros(out_features, n_groups, dtype=scales_dtype), requires_grad=False
        )

        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter("bias", None)

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        if self.weight.dtype not in (torch.uint32, torch.int32):
            return nn.functional.linear(input, self.weight, self.bias)

        kernel = _get_mlx_kernel()

        # affine_qmm_t: y = x @ dequant(w).T
        # x: [..., M, K],  w: [N, K_packed],  scales/qbiases: [N, K//gs]
        # The Metal kernel templates `T` on x.scalar_type() and expects
        # scales & biases in the same dtype, so we must cast.
        output = kernel.affine_qmm_t(
            input,
            self.weight,
            self.scales.to(input.dtype),
            self.qbiases.to(input.dtype),
            self.group_size,
            self.bits,
        )

        if self.bias is not None:
            output = output + self.bias
        return output


def replace_with_mlx_linear(
    model,
    modules_to_not_convert: list[str] | None = None,
    quantization_config=None,
    pre_quantized: bool = False,
):
    """
    replace every eligible ``nn.Linear`` with ``MlxLinear``.

    Args:
        model: the ``PreTrainedModel`` (on the meta device at this point).
        modules_to_not_convert: module names to leave untouched.
        quantization_config: the ``MlxConfig`` instance.
        pre_quantized: ``True`` when loading from a quantized checkpoint.
    """
    if quantization_config.dequantize:
        return model

    bits = quantization_config.bits
    group_size = quantization_config.group_size

    has_been_replaced = False

    for module_name, module in model.named_modules():
        if not should_convert_module(module_name, modules_to_not_convert):
            continue

        if isinstance(module, nn.Linear):
            # Following the FP8 pattern: when pre-quantized, don't pass dtype
            # so the default (torch.uint32) is used → packed uint32 weights.
            # When quantize-on-the-fly, pass dtype=None so the weight is
            # created with full shape in the default float dtype, allowing the
            # loader to fill it before MlxQuantize converts it.
            module_kwargs = {} if pre_quantized else {"dtype": None}

            with torch.device("meta"):
                new_module = MlxLinear(
                    in_features=module.in_features,
                    out_features=module.out_features,
                    bias=module.bias is not None,
                    bits=bits,
                    group_size=group_size,
                    **module_kwargs,
                )

            model.set_submodule(module_name, new_module)
            has_been_replaced = True

    if not has_been_replaced:
        logger.warning(
            "You are loading a model with MLX quantization but no nn.Linear modules were found. "
            "Please double check your model architecture."
        )

    return model

def _affine_quantize_tensor(weight: torch.Tensor, group_size: int, bits: int):
    """
    Quantize a 2-D float weight ``[N, K]`` into packed uint32 + scales + biases.

    Returns ``(w_packed, scales, biases)`` with:
      - ``w_packed``: ``[N, K // (32 // bits)]`` uint32
      - ``scales``:   ``[N, K // group_size]`` float32
      - ``biases``:   ``[N, K // group_size]`` float32
    """
    N, K = weight.shape
    elems_per_int = 32 // bits
    max_val = (1 << bits) - 1
    n_groups = K // group_size

    w_grouped = weight.float().reshape(N, n_groups, group_size)
    w_min = w_grouped.min(dim=-1).values   # [N, n_groups]
    w_max = w_grouped.max(dim=-1).values

    scales = ((w_max - w_min) / max_val).clamp(min=1e-8)
    biases = w_min

    w_int = ((w_grouped - biases.unsqueeze(-1)) / scales.unsqueeze(-1))
    w_int = w_int.round().clamp(0, max_val).to(torch.int32).reshape(N, K)

    # Pack into uint32
    k_packed = K // elems_per_int
    w_packed = torch.zeros(N, k_packed, dtype=torch.int32, device=weight.device)
    for i in range(elems_per_int):
        w_packed |= w_int[:, i::elems_per_int] << (bits * i)

    return w_packed.to(torch.uint32), scales, biases


def _affine_dequantize_tensor(w_packed: torch.Tensor, scales: torch.Tensor, biases: torch.Tensor, group_size: int, bits: int):
    """
    Dequantize a packed uint32 weight ``[N, K_packed]`` back to float.

    Returns a ``[N, K]`` float32 tensor.
    """
    N = w_packed.shape[0]
    elems_per_int = 32 // bits
    max_val = (1 << bits) - 1
    K = w_packed.shape[1] * elems_per_int

    w_packed_i = w_packed.to(torch.int32)
    w_flat = torch.zeros(N, K, dtype=torch.float32, device=w_packed.device)
    for i in range(elems_per_int):
        w_flat[:, i::elems_per_int] = ((w_packed_i >> (bits * i)) & max_val).float()

    w_grouped = w_flat.reshape(N, -1, group_size)
    w_deq = w_grouped * scales.float().unsqueeze(-1) + biases.float().unsqueeze(-1)
    return w_deq.reshape(N, K)


class MlxQuantize(ConversionOps):
    """
    Quantize a full-precision weight tensor into (weight_packed, scales, qbiases).

    Used during quantize-on-the-fly.
    """

    def __init__(self, hf_quantizer):
        self.hf_quantizer = hf_quantizer

    def convert(self, input_dict: dict, **kwargs) -> dict:
        target_key, value = next(iter(input_dict.items()))
        value = value[0] if isinstance(value, list) else value

        bits = self.hf_quantizer.quantization_config.bits
        group_size = self.hf_quantizer.quantization_config.group_size

        w_packed, scales, biases = _affine_quantize_tensor(value, group_size, bits)

        # Derive the sibling key names
        base = target_key.rsplit(".", 1)[0] if "." in target_key else ""
        scale_key = f"{base}.scales" if base else "scales"
        bias_key = f"{base}.qbiases" if base else "qbiases"

        # Store scales/qbiases in the same dtype as the original weight so the
        # Metal kernel (which templates on x.scalar_type()) can use them
        # directly without a per-forward cast.
        orig_dtype = value.dtype  # e.g. bfloat16 for Llama
        return {
            target_key: w_packed,
            scale_key: scales.to(orig_dtype),
            bias_key: biases.to(orig_dtype),
        }


class MlxDequantize(ConversionOps):
    """
    Dequantize (weight_packed, scales, qbiases) back to a full-precision tensor.

    Used when ``dequantize=True`` is set in the config to fall back to a normal
    ``nn.Linear`` on devices without MPS.
    """

    def __init__(self, hf_quantizer):
        self.hf_quantizer = hf_quantizer

    def convert(self, input_dict: dict, full_layer_name: str | None = None, **kwargs) -> dict:
        bits = self.hf_quantizer.quantization_config.bits
        group_size = self.hf_quantizer.quantization_config.group_size

        # The WeightConverter collects tensors matching source_patterns
        quantized = input_dict.get("weight$")
        if quantized is None:
            quantized = next(iter(input_dict.values()))
        quantized = quantized[0] if isinstance(quantized, list) else quantized

        scales = input_dict.get("scales")
        scales = scales[0] if isinstance(scales, list) else scales

        qbiases = input_dict.get("qbiases")
        qbiases = qbiases[0] if isinstance(qbiases, list) else qbiases

        if scales is None or qbiases is None:
            # Only the weight was found; return as-is (probably already float)
            return {full_layer_name: quantized}

        w_deq = _affine_dequantize_tensor(quantized, scales, qbiases, group_size, bits)
        return {full_layer_name: w_deq}
